/var/spool/slurmd/job1126549/slurm_script: line 8: activate: No such file or directory

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:19<00:19, 19.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.46s/it]
wandb: Tracking run with wandb version 0.14.0
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/lustre/gst/xuchunfu/zhangxt/anaconda3/envs/protein_predict/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 29457
  Num Epochs = 50
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 46050
  Number of trainable parameters = 85244354
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/46050 [00:00<?, ?it/s]  0%|          | 1/46050 [00:24<318:03:26, 24.86s/it]wandb: Waiting for W&B process to finish... (failed 1).
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: You can sync this run to the cloud by running:
wandb: wandb sync /lustre/gst/xuchunfu/zhangxt/wandb/offline-run-20230508_200156-dxe401rd
wandb: Find logs at: ./wandb/offline-run-20230508_200156-dxe401rd/logs
Traceback (most recent call last):
  File "/lustre/gst/xuchunfu/zhangxt/ft_plm/ft_token.py", line 151, in <module>
    trainer.train()
  File "/lustre/gst/xuchunfu/zhangxt/anaconda3/envs/protein_predict/lib/python3.8/site-packages/transformers/trainer.py", line 1543, in train
    return inner_training_loop(
  File "/lustre/gst/xuchunfu/zhangxt/anaconda3/envs/protein_predict/lib/python3.8/site-packages/transformers/trainer.py", line 1791, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/lustre/gst/xuchunfu/zhangxt/anaconda3/envs/protein_predict/lib/python3.8/site-packages/transformers/trainer.py", line 2557, in training_step
    loss.backward()
  File "/lustre/gst/xuchunfu/zhangxt/anaconda3/envs/protein_predict/lib/python3.8/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/lustre/gst/xuchunfu/zhangxt/anaconda3/envs/protein_predict/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.74 GiB (GPU 0; 39.41 GiB total capacity; 27.00 GiB already allocated; 6.67 GiB free; 31.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
